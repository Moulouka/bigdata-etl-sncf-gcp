# ğŸ“Š Pipeline ETL Big Data â€” Transport & Open Data

**Master 2 Calcul Scientifique (M2 CS)**  
UniversitÃ© de Reims Champagne-Ardenne

Ce projet consiste Ã  dÃ©velopper un pipeline complet de Data Engineering permettant la collecte, le stockage, la transformation et lâ€™analyse de donnÃ©es issues de diffÃ©rentes APIs publiques.

---

## ğŸ‘¥ Auteurs

- Moulouka Mohamed
- Mohamed Daher
- TraorÃ© Yannick Quentin

---

## ğŸ¯ Objectifs du projet

Le but du projet est de construire une architecture de traitement de donnÃ©es similaire Ã  celle utilisÃ©e en entreprise :

- Collecter automatiquement des donnÃ©es ouvertes
- Les stocker dans un Data Lake
- Les nettoyer et les structurer
- Les charger dans un Data Warehouse
- Produire des indicateurs analytiques

---

## ğŸ§­ Architecture du pipeline

Le pipeline suit une **Medallion Architecture** en trois couches.

### ğŸ¥‰ Bronze â€” Data Lake (Google Cloud Storage)
Stockage des donnÃ©es brutes rÃ©cupÃ©rÃ©es depuis les APIs publiques.  
Aucune transformation nâ€™est appliquÃ©e.

### ğŸ¥ˆ Silver â€” Data Warehouse brut (BigQuery)
DonnÃ©es nettoyÃ©es et structurÃ©es :
- suppression des valeurs manquantes
- typage des colonnes
- normalisation

### ğŸ¥‡ Gold â€” Couche analytique
DonnÃ©es agrÃ©gÃ©es et prÃªtes pour lâ€™analyse mÃ©tier :
- indicateurs statistiques
- tables analytiques
- exploitation dÃ©cisionnelle

---

## ğŸ§­ SchÃ©ma dâ€™architecture

![Architecture du projet](images/architecture.png)

---

## ğŸ“‚ Structure du projet

```
bigdata-etl-sncf-gcp/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 0_test_connection.ipynb
â”‚   â”œâ”€â”€ 1_ingest_to_gcs.ipynb
â”‚   â”œâ”€â”€ 2_load_to_bigquery.ipynb
â”‚   â””â”€â”€ 3_analyze_for_gold.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ gcs_utils.py
â”‚   â””â”€â”€ bq_utils.py
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ report/
â”œâ”€â”€ images/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§° Technologies utilisÃ©es

- Python 3.11
- Pandas
- Google Cloud Platform
- Google Cloud Storage (GCS)
- BigQuery
- APIs Open Data

---

## ğŸ”— Sources de donnÃ©es

- Ãle-de-France MobilitÃ©s Open Data
- Open-Meteo API
- API Gouvernement (jours fÃ©riÃ©s)
- GeoAPI (communes)

---

## âš™ï¸ Installation

Cloner le projet :

```bash
git clone https://github.com/<votre-repo>.git
cd bigdata-etl-sncf-gcp
```

CrÃ©er un environnement virtuel :

```bash
python -m venv venv
```

Activer lâ€™environnement :

### Windows
```bash
venv\Scripts\activate
```

### Linux / Mac
```bash
source venv/bin/activate
```

Installer les dÃ©pendances :

```bash
pip install -r requirements.txt
```

---

## ğŸ” Configuration

CrÃ©er un fichier `.env` Ã  la racine du projet :

```
PROJECT_ID=your-gcp-project-id
GOOGLE_APPLICATION_CREDENTIALS=secrets/service-account.json
BUCKET_NAME=your-gcs-bucket-name
```

---

## â–¶ï¸ ExÃ©cution du pipeline

Lancer Jupyter Notebook :

```bash
jupyter notebook
```

Puis exÃ©cuter les notebooks dans lâ€™ordre :

1. 0_test_connection
2. 1_ingest_to_gcs
3. 2_load_to_bigquery
4. 3_analyze_for_gold

---

## ğŸ“Š RÃ©sultats attendus

Ã€ la fin du pipeline :

- les donnÃ©es sont stockÃ©es dans BigQuery
- les tables sont nettoyÃ©es
- les indicateurs sont calculÃ©s
- la base est prÃªte pour lâ€™analyse mÃ©tier

---

## ğŸ“‘ Rapport

Le rapport dÃ©taillÃ© du projet est disponible ici :

[ğŸ“„ TÃ©lÃ©charger le rapport](report/rapport.pdf)

---

## ğŸ“œ Licence

Projet rÃ©alisÃ© uniquement dans un cadre pÃ©dagogique universitaire.
