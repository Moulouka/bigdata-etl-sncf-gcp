{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557bd78a",
   "metadata": {},
   "source": [
    "# Notebook 2 : Chargement des Données vers BigQuery (LOAD)\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Ce notebook permet de **charger les données depuis Google Cloud Storage (GCS) vers BigQuery** pour créer la couche \"silver\" du pipeline ETL. \n",
    "\n",
    "Les données brutes stockées dans GCS (couche \"bronze\") sont chargées dans BigQuery.\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Avant d'exécuter ce notebook, assurez-vous d'avoir :\n",
    "\n",
    "1. **Exécuté le notebook `1_[EXTRACT]_ingest_to_gcs.ipynb`** pour avoir des données dans GCS\n",
    "2. **Fichier `.env` configuré** avec les variables d'environnement nécessaires\n",
    "3. **Service Account** avec les permissions BigQuery (`BigQuery Data Editor`, `BigQuery Job User`)\n",
    "4. **Packages Python installés** : `google-cloud-bigquery`, `google-cloud-storage`, `pandas`, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962f99c",
   "metadata": {},
   "source": [
    "## 1 - Configuration et Authentification\n",
    "\n",
    "Cette section configure l'environnement et établit la connexion avec BigQuery et GCS.\n",
    "\n",
    "**Étapes :**\n",
    "- Import des bibliothèques nécessaires\n",
    "- Chargement des variables d'environnement depuis `.env`\n",
    "- Authentification avec le Service Account\n",
    "- Création des clients BigQuery et GCS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac220fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] - Configuration et imports terminés\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery, storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "from src.bq_utils import (\n",
    "    load_csv_from_gcs,\n",
    "    load_parquet_from_gcs,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "SA_PATH = ROOT / os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "DATASET_ID = \"silver\"\n",
    "\n",
    "# Authentification\n",
    "creds = service_account.Credentials.from_service_account_file(SA_PATH)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID, credentials=creds)\n",
    "storage_client = storage.Client(project=PROJECT_ID, credentials=creds)\n",
    "\n",
    "print(\"[OK] - Configuration et imports terminés\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1044d",
   "metadata": {},
   "source": [
    "### 1.1 - Création du Dataset BigQuery\n",
    "\n",
    "Création du dataset \"silver\" s'il n'existe pas déjà. Le dataset est l'équivalent d'un schéma dans une base de données relationnelle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c8781b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] - Dataset silver créé\n"
     ]
    }
   ],
   "source": [
    "# Création du dataset s'il n'existe pas\n",
    "dataset_ref = bq_client.dataset(DATASET_ID)\n",
    "try:\n",
    "    bq_client.get_dataset(dataset_ref)\n",
    "    print(f\"[OK] - Dataset {DATASET_ID} existe déjà\")\n",
    "except Exception:\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = \"EU\"\n",
    "    dataset = bq_client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"[OK] - Dataset {DATASET_ID} créé\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c77f9",
   "metadata": {},
   "source": [
    "## 2 - Chargement des Tables de Dimension\n",
    "\n",
    "Les tables de dimension contiennent les données de référence qui seront utilisées pour enrichir les tables de fait. Elles sont généralement stables dans le temps.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 - Table `dim_gare` (Emplacement des Gares)\n",
    "\n",
    "Cette table contient les informations géographiques et descriptives de toutes les gares d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Défini manuellement avec clé primaire `id_gares`\n",
    "- **Types de données** : Géographie (GEOGRAPHY), entiers, chaînes de caractères\n",
    "- **Clé primaire** : `id_gares` (mode REQUIRED)\n",
    "\n",
    "**Note** : Le schéma manuel permet de contrôler précisément les types de données, notamment pour les colonnes géographiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb26fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/emplacement-des-gares-idf/emplacement-des-gares-idf.parquet vers big-data-projet-sncf.silver.gares...\n",
      "[OK] - 1240 lignes chargées dans big-data-projet-sncf.silver.gares\n",
      "[OK] - Taille: 0.62 MB\n",
      "[OK] - Clé primaire: id_gares\n"
     ]
    }
   ],
   "source": [
    "# Chargement direct depuis GCS (bronze) vers BigQuery (silver) avec schéma manuel\n",
    "gcs_path_gares = \"bronze/emplacement-des-gares-idf/emplacement-des-gares-idf.parquet\"\n",
    "\n",
    "# Définition du schéma manuel avec clé primaire (id_gares)\n",
    "schema_gares = [\n",
    "    bigquery.SchemaField(\"geo_point_2d\", \"GEOGRAPHY\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"geo_shape\", \"GEOGRAPHY\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_gares\", \"INTEGER\", mode=\"REQUIRED\", description=\"Clé primaire\"),\n",
    "    bigquery.SchemaField(\"nom_gares\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_so_gar\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_su_gar\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_ref_zdc\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_zdc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_ref_zda\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_zda\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"idrefliga\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"idrefligc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"res_com\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"indice_lig\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mode\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tertrain\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"terrer\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"termetro\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tertram\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"terval\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"exploitant\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"idf\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"principal\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"x\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"y\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"picto\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_iv\", \"STRING\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "table_id = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_gares,\n",
    "    table_name=\"gares\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=schema_gares,\n",
    "    primary_key=\"id_gares\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df54d0",
   "metadata": {},
   "source": [
    "### 2.2 - Vérification de la Table `dim_gare`\n",
    "\n",
    "Après le chargement, on vérifie que les données ont été correctement chargées en :\n",
    "- Affichant le nombre de lignes\n",
    "- Listant les colonnes et leurs types\n",
    "- Afficant un aperçu des données (5 premières lignes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6688f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] - Nombre total de lignes: 1240\n",
      "[OK] - Colonnes:\n",
      "  - geo_point_2d: GEOGRAPHY\n",
      "  - geo_shape: GEOGRAPHY\n",
      "  - id_gares: INTEGER\n",
      "  - nom_gares: STRING\n",
      "  - nom_so_gar: STRING\n",
      "  - nom_su_gar: STRING\n",
      "  - id_ref_zdc: INTEGER\n",
      "  - nom_zdc: STRING\n",
      "  - id_ref_zda: INTEGER\n",
      "  - nom_zda: STRING\n",
      "  - idrefliga: STRING\n",
      "  - idrefligc: STRING\n",
      "  - res_com: STRING\n",
      "  - indice_lig: STRING\n",
      "  - mode: STRING\n",
      "  - tertrain: STRING\n",
      "  - terrer: STRING\n",
      "  - termetro: STRING\n",
      "  - tertram: STRING\n",
      "  - terval: STRING\n",
      "  - exploitant: STRING\n",
      "  - idf: INTEGER\n",
      "  - principal: INTEGER\n",
      "  - x: FLOAT\n",
      "  - y: FLOAT\n",
      "  - picto: STRING\n",
      "  - nom_iv: STRING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\Master 2 - CS\\BigData\\ProjetFinal\\m2-univ-reims-sep-cs-etl-sncf-gcp\\venv\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] - Aperçu des données (5 premières lignes):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_point_2d</th>\n",
       "      <th>geo_shape</th>\n",
       "      <th>id_gares</th>\n",
       "      <th>nom_gares</th>\n",
       "      <th>nom_so_gar</th>\n",
       "      <th>nom_su_gar</th>\n",
       "      <th>id_ref_zdc</th>\n",
       "      <th>nom_zdc</th>\n",
       "      <th>id_ref_zda</th>\n",
       "      <th>nom_zda</th>\n",
       "      <th>...</th>\n",
       "      <th>termetro</th>\n",
       "      <th>tertram</th>\n",
       "      <th>terval</th>\n",
       "      <th>exploitant</th>\n",
       "      <th>idf</th>\n",
       "      <th>principal</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>picto</th>\n",
       "      <th>nom_iv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT(2.49271300288158 48.5661178207362)</td>\n",
       "      <td>POINT(2.49271300288158 48.5661178207362)</td>\n",
       "      <td>214</td>\n",
       "      <td>Le Coudray-Montceaux</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59953</td>\n",
       "      <td>Le Coudray-Montceaux</td>\n",
       "      <td>45824</td>\n",
       "      <td>Le Coudray-Montceaux</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SNCF</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>662571.2287</td>\n",
       "      <td>6.829664e+06</td>\n",
       "      <td>{\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...</td>\n",
       "      <td>Le Coudray-Montceaux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT(2.45257339227245 48.634268312369)</td>\n",
       "      <td>POINT(2.45257339227245 48.634268312369)</td>\n",
       "      <td>276</td>\n",
       "      <td>Évry</td>\n",
       "      <td>Val de Seine</td>\n",
       "      <td>None</td>\n",
       "      <td>60522</td>\n",
       "      <td>Évry</td>\n",
       "      <td>45745</td>\n",
       "      <td>Évry</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SNCF</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>659662.2083</td>\n",
       "      <td>6.837260e+06</td>\n",
       "      <td>{\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...</td>\n",
       "      <td>Évry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT(2.40113389442958 48.2935917687313)</td>\n",
       "      <td>POINT(2.40113389442958 48.2935917687313)</td>\n",
       "      <td>955</td>\n",
       "      <td>Malesherbes</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>411486</td>\n",
       "      <td>Malesherbes</td>\n",
       "      <td>411485</td>\n",
       "      <td>Malesherbes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SNCF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>655584.6925</td>\n",
       "      <td>6.799420e+06</td>\n",
       "      <td>{\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...</td>\n",
       "      <td>Malesherbes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT(2.65499028214189 48.5272433214602)</td>\n",
       "      <td>POINT(2.65499028214189 48.5272433214602)</td>\n",
       "      <td>530</td>\n",
       "      <td>Melun</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>61926</td>\n",
       "      <td>Melun</td>\n",
       "      <td>47909</td>\n",
       "      <td>Melun</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SNCF</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>674525.4340</td>\n",
       "      <td>6.825278e+06</td>\n",
       "      <td>{\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...</td>\n",
       "      <td>Melun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT(2.47946916960178 48.573807793559)</td>\n",
       "      <td>POINT(2.47946916960178 48.573807793559)</td>\n",
       "      <td>433</td>\n",
       "      <td>Le Plessis-Chenet</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>60006</td>\n",
       "      <td>Le Plessis Chenet</td>\n",
       "      <td>45820</td>\n",
       "      <td>Le Plessis Chenet</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SNCF</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>661599.7184</td>\n",
       "      <td>6.830526e+06</td>\n",
       "      <td>{\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...</td>\n",
       "      <td>Le Plessis-Chenet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               geo_point_2d  \\\n",
       "0  POINT(2.49271300288158 48.5661178207362)   \n",
       "1   POINT(2.45257339227245 48.634268312369)   \n",
       "2  POINT(2.40113389442958 48.2935917687313)   \n",
       "3  POINT(2.65499028214189 48.5272433214602)   \n",
       "4   POINT(2.47946916960178 48.573807793559)   \n",
       "\n",
       "                                  geo_shape  id_gares             nom_gares  \\\n",
       "0  POINT(2.49271300288158 48.5661178207362)       214  Le Coudray-Montceaux   \n",
       "1   POINT(2.45257339227245 48.634268312369)       276                  Évry   \n",
       "2  POINT(2.40113389442958 48.2935917687313)       955           Malesherbes   \n",
       "3  POINT(2.65499028214189 48.5272433214602)       530                 Melun   \n",
       "4   POINT(2.47946916960178 48.573807793559)       433     Le Plessis-Chenet   \n",
       "\n",
       "     nom_so_gar nom_su_gar  id_ref_zdc               nom_zdc  id_ref_zda  \\\n",
       "0          None       None       59953  Le Coudray-Montceaux       45824   \n",
       "1  Val de Seine       None       60522                  Évry       45745   \n",
       "2          None       None      411486           Malesherbes      411485   \n",
       "3          None       None       61926                 Melun       47909   \n",
       "4          None       None       60006     Le Plessis Chenet       45820   \n",
       "\n",
       "                nom_zda  ... termetro tertram terval exploitant idf principal  \\\n",
       "0  Le Coudray-Montceaux  ...        0       0      0       SNCF   1         0   \n",
       "1                  Évry  ...        0       0      0       SNCF   1         0   \n",
       "2           Malesherbes  ...        0       0      0       SNCF   0         1   \n",
       "3                 Melun  ...        0       0      0       SNCF   1         1   \n",
       "4     Le Plessis Chenet  ...        0       0      0       SNCF   1         0   \n",
       "\n",
       "             x             y  \\\n",
       "0  662571.2287  6.829664e+06   \n",
       "1  659662.2083  6.837260e+06   \n",
       "2  655584.6925  6.799420e+06   \n",
       "3  674525.4340  6.825278e+06   \n",
       "4  661599.7184  6.830526e+06   \n",
       "\n",
       "                                               picto                nom_iv  \n",
       "0  {\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...  Le Coudray-Montceaux  \n",
       "1  {\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...                  Évry  \n",
       "2  {\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...           Malesherbes  \n",
       "3  {\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...                 Melun  \n",
       "4  {\"thumbnail\": true, \"filename\": \"RER_D.svg\", \"...     Le Plessis-Chenet  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vérification de la table chargée\n",
    "table = bq_client.get_table(table_id)\n",
    "print(f\"[OK] - Nombre total de lignes: {table.num_rows}\")\n",
    "print(f\"[OK] - Colonnes:\")\n",
    "for field in table.schema:\n",
    "    print(f\"  - {field.name}: {field.field_type}\")\n",
    "\n",
    "# Requête simple pour vérifier les données et convertir en DataFrame pandas\n",
    "query = f\"SELECT * FROM `{table_id}` LIMIT 5\"\n",
    "results = bq_client.query(query).result()\n",
    "df = results.to_dataframe()\n",
    "\n",
    "print(f\"\\n[OK] - Aperçu des données (5 premières lignes):\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2937e",
   "metadata": {},
   "source": [
    "### 2.3 - Table `dim_ligne` (Référentiel des Lignes)\n",
    "\n",
    "Cette table contient les informations sur toutes les lignes de transport en commun d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Contenu** : Informations sur les lignes (numéros, noms, types de transport, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd796e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/referentiel-des-lignes/referentiel-des-lignes.parquet vers big-data-projet-sncf.silver.dim_ligne...\n",
      "[OK] - 2116 lignes chargées dans big-data-projet-sncf.silver.dim_ligne\n",
      "[OK] - Taille: 0.57 MB\n"
     ]
    }
   ],
   "source": [
    "gcs_path_lignes = \"bronze/referentiel-des-lignes/referentiel-des-lignes.parquet\"\n",
    "table_id_lignes = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_lignes,\n",
    "    table_name=\"dim_ligne\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    primary_key=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a769c",
   "metadata": {},
   "source": [
    "### 2.4 - Table `dim_arret` (Référentiel des Arrêts)\n",
    "\n",
    "Cette table contient les informations sur tous les arrêts de transport en commun d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Contenu** : Informations sur les arrêts (noms, coordonnées, lignes desservies, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e16ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/arrets/arrets.parquet vers big-data-projet-sncf.silver.dim_arret...\n",
      "[OK] - 38368 lignes chargées dans big-data-projet-sncf.silver.dim_arret\n",
      "[OK] - Taille: 6.30 MB\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "gcs_path_arrets = \"bronze/arrets/arrets.parquet\"\n",
    "table_id_arrets = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_arrets,\n",
    "    table_name=\"dim_arret\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    primary_key=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3adda",
   "metadata": {},
   "source": [
    "### 2.5 - Table `dim_transporteur` (Liste des Transporteurs)\n",
    "\n",
    "Cette table contient les informations sur tous les transporteurs (opérateurs de transport) d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Contenu** : Informations sur les transporteurs (noms, codes, types de transport, etc.)\n",
    "\n",
    "**Note** : Cette table de dimension permet d'identifier les différents opérateurs de transport qui gèrent les lignes et arrêts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9e46fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/liste-transporteurs/liste-transporteurs.parquet vers big-data-projet-sncf.silver.dim_transporteur...\n",
      "[OK] - 54 lignes chargées dans big-data-projet-sncf.silver.dim_transporteur\n",
      "[OK] - Taille: 0.01 MB\n"
     ]
    }
   ],
   "source": [
    "gcs_path_transporteurs = \"bronze/liste-transporteurs/liste-transporteurs.parquet\"\n",
    "table_id_transporteurs = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_transporteurs,\n",
    "    table_name=\"dim_transporteur\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    primary_key=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9d463",
   "metadata": {},
   "source": [
    "### 2.5 - Table `dim_vacances_scolaires` (Calendrier des Vacances Scolaires)\n",
    "\n",
    "Cette table contient les périodes de vacances scolaires pour différentes zones et années.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : CSV (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Encodage** : UTF-8\n",
    "- **Séparateur** : Point-virgule (`;`)\n",
    "- **Contenu** : Dates de début/fin de vacances, zones, années, etc.\n",
    "\n",
    "**Note** : Pour les fichiers CSV, il est important de spécifier l'encodage et le séparateur pour éviter les erreurs de parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f58b4660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/vacances-scolaires/vacances_scolaires.csv vers big-data-projet-sncf.silver.dim_vacances_scolaires...\n",
      "[OK] - 2320 lignes chargées dans big-data-projet-sncf.silver.dim_vacances_scolaires\n",
      "[OK] - Taille: 0.16 MB\n"
     ]
    }
   ],
   "source": [
    "gcs_path_vacances = \"bronze/vacances-scolaires/vacances_scolaires.csv\"\n",
    "table_id_vacances = load_csv_from_gcs(\n",
    "    gcs_path=gcs_path_vacances,\n",
    "    table_name=\"dim_vacances_scolaires\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    skip_leading_rows=1,\n",
    "    encoding=\"utf-8\",\n",
    "    sep=\";\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e9487",
   "metadata": {},
   "source": [
    "## 3 - Chargement des Tables de Fait\n",
    "\n",
    "Les tables de fait contiennent les mesures et événements métier. Ici, nous chargeons les données historiques de validations des titres de transport.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 - Configuration pour les Fichiers de Validation\n",
    "\n",
    "Les fichiers de validation historiques ont des formats différents selon les années :\n",
    "- **Encodages variés** : UTF-8, UTF-16LE, Latin-1\n",
    "- **Séparateurs variés** : Tabulation (`\\t`), point-virgule (`;`)\n",
    "- **Extensions variées** : `.txt`, `.csv`\n",
    "\n",
    "Ce dictionnaire de configuration permet de spécifier les paramètres corrects pour chaque fichier.\n",
    "\n",
    "**Note importante** : Le fichier `2023_S2_NB_FER.txt` utilise l'encodage UTF-16LE, qui n'est pas supporté directement par BigQuery. La fonction `load_csv_from_gcs` convertit automatiquement ce fichier en UTF-8 avant le chargement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d04b8",
   "metadata": {},
   "source": [
    "### 3.2 - Chargement des Fichiers de Validation\n",
    "\n",
    "Cette section charge tous les fichiers de validation historiques depuis GCS vers BigQuery.\n",
    "\n",
    "**Processus :**\n",
    "1. Parcourt le dictionnaire de configuration\n",
    "2. Recherche chaque fichier dans GCS\n",
    "3. Charge le fichier avec les paramètres appropriés (encodage, séparateur, format de date)\n",
    "4. Crée une table séparée pour chaque fichier (ex: `fact_validations_2015s1_nb_fer_csv`)\n",
    "\n",
    "**Gestion spéciale :**\n",
    "- **Fichiers UTF-16LE** : Conversion automatique en UTF-8 (nécessite `storage_client`)\n",
    "- **Format de date** : `DD/MM/YYYY` (format BigQuery pour les dates françaises)\n",
    "- **Schéma** : Auto-détecté (toutes les colonnes en STRING pour éviter les erreurs de parsing)\n",
    "\n",
    "**Durée estimée** : Plusieurs minutes selon le nombre et la taille des fichiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c38873",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_rf_config = {\n",
    "    \"2015S1_NB_FER.csv\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \";\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2015S2_NB_FER.csv\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \";\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2016S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2016S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2017S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2017_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2018_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2019_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2019_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2020_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2020_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2021_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2021_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2022_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2022_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \";\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2023_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-16le\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2024_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e86c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2015/data-rf-2015/2015S1_NB_FER.csv vers big-data-projet-sncf.silver.fact_validations_2015s1_nb_fer_csv...\n",
      "[OK] - 755989 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2015s1_nb_fer_csv\n",
      "[OK] - Taille: 45.93 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2015/data-rf-2015/2015S2_NB_FER.csv vers big-data-projet-sncf.silver.fact_validations_2015s2_nb_fer_csv...\n",
      "[OK] - 778747 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2015s2_nb_fer_csv\n",
      "[OK] - Taille: 47.28 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2016/data-rf-2016/2016S1_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2016s1_nb_fer_txt...\n",
      "[OK] - 779712 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2016s1_nb_fer_txt\n",
      "[OK] - Taille: 52.96 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2016/data-rf-2016/2016S2_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2016s2_nb_fer_txt...\n",
      "[OK] - 774421 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2016s2_nb_fer_txt\n",
      "[OK] - Taille: 52.59 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2017/data-rf-2017/2017S1_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2017s1_nb_fer_txt...\n",
      "[OK] - 780270 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2017s1_nb_fer_txt\n",
      "[OK] - Taille: 53.02 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2017/data-rf-2017/2017_S2_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2017_s2_nb_fer_txt...\n",
      "[OK] - 825698 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2017_s2_nb_fer_txt\n",
      "[OK] - Taille: 52.00 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2018/data-rf-2018/2018_S1_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2018_s1_nb_fer_txt...\n",
      "[OK] - 866694 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2018_s1_nb_fer_txt\n",
      "[OK] - Taille: 54.94 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2019/data-rf-2019/2019_S1_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2019_s1_nb_fer_txt...\n",
      "[OK] - 934851 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2019_s1_nb_fer_txt\n",
      "[OK] - Taille: 59.47 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2019/data-rf-2019/2019_S2_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2019_s2_nb_fer_txt...\n",
      "[OK] - 953454 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2019_s2_nb_fer_txt\n",
      "[OK] - Taille: 60.98 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2020/data-rf-2020/2020_S1_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2020_s1_nb_fer_txt...\n",
      "[OK] - 887493 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2020_s1_nb_fer_txt\n",
      "[OK] - Taille: 56.89 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2020/data-rf-2020/2020_S2_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2020_s2_nb_fer_txt...\n",
      "[OK] - 1054012 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2020_s2_nb_fer_txt\n",
      "[OK] - Taille: 67.52 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2021/data-rf-2021/2021_S1_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2021_s1_nb_fer_txt...\n",
      "[OK] - 1064019 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2021_s1_nb_fer_txt\n",
      "[OK] - Taille: 68.20 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2021/data-rf-2021/2021_S2_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2021_s2_nb_fer_txt...\n",
      "[OK] - 1084281 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2021_s2_nb_fer_txt\n",
      "[OK] - Taille: 69.54 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2022/data-rf-2022/2022_S1_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2022_s1_nb_fer_txt...\n",
      "[OK] - 1088334 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2022_s1_nb_fer_txt\n",
      "[OK] - Taille: 69.80 MB\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2022/data-rf-2022/2022_S2_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2022_s2_nb_fer_txt...\n",
      "[OK] - 1105947 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2022_s2_nb_fer_txt\n",
      "[OK] - Taille: 69.08 MB\n",
      "\n",
      "[...] - Conversion UTF-16LE -> UTF-8 pour bronze/histo-validations-reseau-ferre/2023/data-rf-2023/2023_S2_NB_FER.txt...\n",
      "[OK] - Fichier converti et uploadé vers bronze/histo-validations-reseau-ferre/2023/data-rf-2023/2023_S2_NB_FER.txt.utf8\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2023/data-rf-2023/2023_S2_NB_FER.txt.utf8 vers big-data-projet-sncf.silver.fact_validations_2023_s2_nb_fer_txt...\n",
      "[OK] - 849596 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2023_s2_nb_fer_txt\n",
      "[OK] - Taille: 62.86 MB\n",
      "[OK] - Fichier temporaire bronze/histo-validations-reseau-ferre/2023/data-rf-2023/2023_S2_NB_FER.txt.utf8 supprimé\n",
      "\n",
      "[...] - Chargement de gs://big-data-projet-bucket/bronze/histo-validations-reseau-ferre/2024/data-rf-2024/2024_S1_NB_FER.txt vers big-data-projet-sncf.silver.fact_validations_2024_s1_nb_fer_txt...\n",
      "[OK] - 859043 lignes chargées dans big-data-projet-sncf.silver.fact_validations_2024_s1_nb_fer_txt\n",
      "[OK] - Taille: 63.60 MB\n"
     ]
    }
   ],
   "source": [
    "# Charger tous les fichiers de validation depuis GCS vers BigQuery\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Parcourir tous les fichiers dans la configuration\n",
    "for filename, config in load_rf_config.items():\n",
    "    # Chercher le fichier dans GCS\n",
    "    blobs = list(bucket.list_blobs(prefix=\"bronze/histo-validations-reseau-ferre/\"))\n",
    "    \n",
    "    # Trouver le blob correspondant\n",
    "    blob = None\n",
    "    for b in blobs:\n",
    "        if b.name.endswith(filename):\n",
    "            blob = b\n",
    "            break\n",
    "    \n",
    "    if blob is None:\n",
    "        print(f\"[SKIP] - {filename} (non trouvé dans GCS)\")\n",
    "        continue\n",
    "    \n",
    "    gcs_path = blob.name\n",
    "    table_name = f\"fact_validations_{filename.replace('.', '_').replace('-', '_').lower()}\"\n",
    "    \n",
    "    sep = config[\"sep\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "    \n",
    "    # Convertir \"\\t\" en tabulation réelle si nécessaire\n",
    "    if sep == \"\\\\t\":\n",
    "        sep = \"\\t\"\n",
    "    \n",
    "    # Utiliser la fonction load_csv_from_gcs avec le schéma unifié (toutes les colonnes en STRING)\n",
    "    table_id = load_csv_from_gcs(\n",
    "        gcs_path=gcs_path,\n",
    "        table_name=table_name,\n",
    "        bq_client=bq_client,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        schema=None,\n",
    "        skip_leading_rows=config.get(\"skip_rows\", 1),\n",
    "        encoding=encoding,\n",
    "        sep=sep,\n",
    "        date_format=\"DD/MM/YYYY\",  # Format BigQuery pour les dates\n",
    "        storage_client=storage_client,  # Requis pour la conversion UTF-16LE\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
